import lightgbm as lgb
import pandas as pd
from sklearn.metrics import roc_auc_score, log_loss
import os

class AccuracyGuard:
    def __init__(self, validation_df):
        self.val_df = validation_df
        # Assume last column or specific column is target
        # For simplicity, we assume preprocessing is done and 'target' exists
        self.X = validation_df.drop(columns=['target', 'date', 'race_id', 'rank', 'result_rank'], errors='ignore')
        self.y = validation_df['target']
        # Filter only numeric/features used in model
        # Real implementation would need feature name alignment.
        # Here we trust the columns match.

    def compare(self, current_model_path, new_model_obj):
        """
        Returns True if new_model is safe to deploy (better or similar).
        """
        if not os.path.exists(current_model_path):
            print("No existing model. Safe to deploy.")
            return True

        current_model = lgb.Booster(model_file=current_model_path)
        
        # Eval Current
        preds_curr = current_model.predict(self.X)
        score_curr = roc_auc_score(self.y, preds_curr)
        
        # Eval New
        preds_new = new_model_obj.predict(self.X)
        score_new = roc_auc_score(self.y, preds_new)
        
        print(f"ğŸ›¡ï¸ Guard: Current AUC={score_curr:.4f} vs New AUC={score_new:.4f}")
        
        # Threshold: Don't allow drop more than 1% relative
        if score_new >= score_curr * 0.99:
            print("âœ… New model passed.")
            return True
        else:
            print("âŒ New model rejected (Performance Degradation).")
            return False
